{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from six.moves import range\n",
    "from PIL import Image\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/michaelcai/GANtor-Arts-Center/src/code/')\n",
    "\n",
    "import numpy as np\n",
    "import torchfile\n",
    "\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "from miscc.utils import mkdir_p\n",
    "from miscc.utils import weights_init\n",
    "from miscc.utils import save_img_results, save_model\n",
    "from miscc.utils import KL_loss\n",
    "from miscc.utils import compute_discriminator_loss, compute_generator_loss\n",
    "\n",
    "from tensorboard import summary\n",
    "from tensorboardX import FileWriter\n",
    "\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(0)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michaelcai/GANtor-Arts-Center/src/code/miscc/config.py:99: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from:  ../../../results/wikiart_stageII_2019_06_01_06_15_03/Model/netG_epoch_80.pth\n",
      "STAGE2_G(\n",
      "  (STAGE1_G): STAGE1_G(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=110, out_features=24576, bias=False)\n",
      "      (1): BatchNorm1d(24576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (upsample1): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (upsample2): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (upsample3): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (upsample4): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "      (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (img): Sequential(\n",
      "      (0): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (hr_joint): Sequential(\n",
      "    (0): Conv2d(778, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (residual): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (upsample1): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (upsample2): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (upsample3): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (upsample4): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (img): Sequential(\n",
      "    (0): Conv2d(48, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "odict_keys(['STAGE1_G.fc.0.weight', 'STAGE1_G.fc.1.weight', 'STAGE1_G.fc.1.bias', 'STAGE1_G.fc.1.running_mean', 'STAGE1_G.fc.1.running_var', 'STAGE1_G.fc.1.num_batches_tracked', 'STAGE1_G.upsample1.1.weight', 'STAGE1_G.upsample1.2.weight', 'STAGE1_G.upsample1.2.bias', 'STAGE1_G.upsample1.2.running_mean', 'STAGE1_G.upsample1.2.running_var', 'STAGE1_G.upsample1.2.num_batches_tracked', 'STAGE1_G.upsample2.1.weight', 'STAGE1_G.upsample2.2.weight', 'STAGE1_G.upsample2.2.bias', 'STAGE1_G.upsample2.2.running_mean', 'STAGE1_G.upsample2.2.running_var', 'STAGE1_G.upsample2.2.num_batches_tracked', 'STAGE1_G.upsample3.1.weight', 'STAGE1_G.upsample3.2.weight', 'STAGE1_G.upsample3.2.bias', 'STAGE1_G.upsample3.2.running_mean', 'STAGE1_G.upsample3.2.running_var', 'STAGE1_G.upsample3.2.num_batches_tracked', 'STAGE1_G.upsample4.1.weight', 'STAGE1_G.upsample4.2.weight', 'STAGE1_G.upsample4.2.bias', 'STAGE1_G.upsample4.2.running_mean', 'STAGE1_G.upsample4.2.running_var', 'STAGE1_G.upsample4.2.num_batches_tracked', 'STAGE1_G.img.0.weight', 'encoder.0.weight', 'encoder.2.weight', 'encoder.3.weight', 'encoder.3.bias', 'encoder.3.running_mean', 'encoder.3.running_var', 'encoder.3.num_batches_tracked', 'encoder.5.weight', 'encoder.6.weight', 'encoder.6.bias', 'encoder.6.running_mean', 'encoder.6.running_var', 'encoder.6.num_batches_tracked', 'hr_joint.0.weight', 'hr_joint.1.weight', 'hr_joint.1.bias', 'hr_joint.1.running_mean', 'hr_joint.1.running_var', 'hr_joint.1.num_batches_tracked', 'residual.0.block.0.weight', 'residual.0.block.1.weight', 'residual.0.block.1.bias', 'residual.0.block.1.running_mean', 'residual.0.block.1.running_var', 'residual.0.block.1.num_batches_tracked', 'residual.0.block.3.weight', 'residual.0.block.4.weight', 'residual.0.block.4.bias', 'residual.0.block.4.running_mean', 'residual.0.block.4.running_var', 'residual.0.block.4.num_batches_tracked', 'residual.1.block.0.weight', 'residual.1.block.1.weight', 'residual.1.block.1.bias', 'residual.1.block.1.running_mean', 'residual.1.block.1.running_var', 'residual.1.block.1.num_batches_tracked', 'residual.1.block.3.weight', 'residual.1.block.4.weight', 'residual.1.block.4.bias', 'residual.1.block.4.running_mean', 'residual.1.block.4.running_var', 'residual.1.block.4.num_batches_tracked', 'upsample1.1.weight', 'upsample1.2.weight', 'upsample1.2.bias', 'upsample1.2.running_mean', 'upsample1.2.running_var', 'upsample1.2.num_batches_tracked', 'upsample2.1.weight', 'upsample2.2.weight', 'upsample2.2.bias', 'upsample2.2.running_mean', 'upsample2.2.running_var', 'upsample2.2.num_batches_tracked', 'upsample3.1.weight', 'upsample3.2.weight', 'upsample3.2.bias', 'upsample3.2.running_mean', 'upsample3.2.running_var', 'upsample3.2.num_batches_tracked', 'upsample4.1.weight', 'upsample4.2.weight', 'upsample4.2.bias', 'upsample4.2.running_mean', 'upsample4.2.running_var', 'upsample4.2.num_batches_tracked', 'img.0.weight'])\n"
     ]
    }
   ],
   "source": [
    "cfg_from_file('../cfg/wikiart_s2_v2.yml')\n",
    "\n",
    "from modelv2 import STAGE1_G, STAGE2_G, STAGE2_D\n",
    "\n",
    "Stage1_G = STAGE1_G()\n",
    "\n",
    "netG = STAGE2_G(Stage1_G)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# stage_1_file = '../../../results/wikiart_stageI_2019_05_29_16_16_57/Model/netG_epoch_90.pth'\n",
    "stage_2_file = '../../../results/wikiart_stageII_2019_06_01_06_15_03/Model/netG_epoch_80.pth'\n",
    "# stage_2_file = '../../saved_models/netG_epoch_60.pth'\n",
    "state_dict = torch.load(stage_2_file, map_location=lambda storage, loc: storage)\n",
    "netG.load_state_dict(state_dict)\n",
    "print('Load from: ', stage_2_file)\n",
    "\n",
    "# state_dict = torch.load(stage_2_file,map_location=lambda storage, loc: storage)\n",
    "# netG.STAGE1_G.load_state_dict(state_dict)\n",
    "# print('Load from: ', stage_1_file)\n",
    "# netG.eval()\n",
    "# netG.eval()\n",
    "\n",
    "if cfg.CUDA:\n",
    "    netG.cuda()\n",
    "print(netG)\n",
    "print(netG.state_dict().keys())\n",
    "\n",
    "# netG.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    netG = netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'abstract_painting', 1: 'cityscape', 2: 'genre_painting', 3: 'illustration', 4: 'landscape', 5: 'nude_painting', 6: 'portrait', 7: 'religious_painting', 8: 'sketch_and_study', 9: 'still_life'}\n"
     ]
    }
   ],
   "source": [
    "folders = {0: 'abstract_painting', \n",
    "           1: 'cityscape', \n",
    "           2: 'genre_painting', \n",
    "           3: 'illustration', \n",
    "           4: 'landscape', \n",
    "           5: 'nude_painting', \n",
    "           6: 'portrait',\n",
    "           7: 'religious_painting',\n",
    "           8: 'sketch_and_study',\n",
    "           9: 'still_life'}\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = './Generated_Images/'\n",
    "\n",
    "# nz = 100\n",
    "# batch_size = 16\n",
    "# embedding_size = 10\n",
    "\n",
    "\n",
    "# for i in range(0, embedding_size * 5):\n",
    "#     noise = Variable(torch.FloatTensor(batch_size, nz))\n",
    "#     noise = noise.cuda()\n",
    "\n",
    "#     noise.data.normal_(0, 1)\n",
    "#     text_embeddings = torch.zeros((batch_size, embedding_size))\n",
    "#     text_embeddings[:, i % 10] = 1\n",
    "\n",
    "#     inputs = (text_embeddings, noise)\n",
    "\n",
    "#     lr_fake, fake = nn.parallel.data_parallel(netG, inputs, [0])\n",
    "#     print(fake.size())\n",
    "\n",
    "# #     vutils.save_image(\n",
    "# #                 fake.data, '%s/fake_samples_cat_%i.png' %\n",
    "# #                 (image_dir, i), normalize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# netG.eval()\n",
    "image_dir = './Generated_Images/'\n",
    "\n",
    "nz = 100\n",
    "batch_size = 16\n",
    "embedding_size = 10\n",
    "\n",
    "\n",
    "for i in range(0, embedding_size):\n",
    "    noise = Variable(torch.FloatTensor(batch_size, nz))\n",
    "    noise = noise.cuda()\n",
    "\n",
    "    noise.data.normal_(0, 1)\n",
    "    text_embeddings = torch.zeros((batch_size, embedding_size))\n",
    "    text_embeddings[:, i] = 1\n",
    "\n",
    "    inputs = (text_embeddings, noise)\n",
    "\n",
    "    lr_fake, fake = nn.parallel.data_parallel(netG, inputs, [0])\n",
    "    print(fake.size())\n",
    "\n",
    "    vutils.save_image(\n",
    "                fake.data, '%s/fake_samples_cat_%i.png' %\n",
    "                (image_dir, i), normalize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter num 0\n",
      "Iter num 1\n",
      "Iter num 2\n",
      "Iter num 3\n",
      "Iter num 4\n",
      "Iter num 5\n",
      "Iter num 6\n",
      "Iter num 7\n",
      "Iter num 8\n",
      "Iter num 9\n",
      "Iter num 10\n",
      "Iter num 11\n",
      "Iter num 12\n",
      "Iter num 13\n",
      "Iter num 14\n",
      "Iter num 15\n",
      "Iter num 16\n",
      "Iter num 17\n",
      "Iter num 18\n",
      "Iter num 19\n",
      "Iter num 20\n",
      "Iter num 21\n",
      "Iter num 22\n",
      "Iter num 23\n",
      "Iter num 24\n",
      "Iter num 25\n",
      "Iter num 26\n",
      "Iter num 27\n",
      "Iter num 28\n",
      "Iter num 29\n",
      "Iter num 30\n",
      "Iter num 31\n",
      "Iter num 32\n",
      "Iter num 33\n",
      "Iter num 34\n",
      "Iter num 35\n",
      "Iter num 36\n",
      "Iter num 37\n",
      "Iter num 38\n",
      "Iter num 39\n",
      "Iter num 40\n",
      "Iter num 41\n",
      "Iter num 42\n",
      "Iter num 43\n",
      "Iter num 44\n",
      "Iter num 45\n",
      "Iter num 46\n",
      "Iter num 47\n",
      "Iter num 48\n",
      "Iter num 49\n",
      "Iter num 50\n",
      "Iter num 51\n",
      "Iter num 52\n",
      "Iter num 53\n",
      "Iter num 54\n",
      "Iter num 55\n",
      "Iter num 56\n",
      "Iter num 57\n",
      "Iter num 58\n",
      "Iter num 59\n",
      "Iter num 60\n",
      "Iter num 61\n",
      "Iter num 62\n",
      "Iter num 63\n",
      "Iter num 64\n",
      "Iter num 65\n",
      "Iter num 66\n",
      "Iter num 67\n",
      "Iter num 68\n",
      "Iter num 69\n",
      "Iter num 70\n",
      "Iter num 71\n",
      "Iter num 72\n",
      "Iter num 73\n",
      "Iter num 74\n",
      "Iter num 75\n",
      "Iter num 76\n",
      "Iter num 77\n",
      "Iter num 78\n",
      "Iter num 79\n",
      "Iter num 80\n",
      "Iter num 81\n",
      "Iter num 82\n",
      "Iter num 83\n",
      "Iter num 84\n",
      "Iter num 85\n",
      "Iter num 86\n",
      "Iter num 87\n",
      "Iter num 88\n",
      "Iter num 89\n",
      "Iter num 90\n",
      "Iter num 91\n",
      "Iter num 92\n",
      "Iter num 93\n",
      "Iter num 94\n",
      "Iter num 95\n",
      "Iter num 96\n",
      "Iter num 97\n",
      "Iter num 98\n",
      "Iter num 99\n",
      "Iter num 100\n",
      "Iter num 101\n",
      "Iter num 102\n",
      "Iter num 103\n",
      "Iter num 104\n",
      "Iter num 105\n",
      "Iter num 106\n",
      "Iter num 107\n",
      "Iter num 108\n",
      "Iter num 109\n",
      "Iter num 110\n",
      "Iter num 111\n",
      "Iter num 112\n",
      "Iter num 113\n",
      "Iter num 114\n",
      "Iter num 115\n",
      "Iter num 116\n",
      "Iter num 117\n",
      "Iter num 118\n",
      "Iter num 119\n",
      "Iter num 120\n",
      "Iter num 121\n",
      "Iter num 122\n",
      "Iter num 123\n",
      "Iter num 124\n",
      "Iter num 125\n",
      "Iter num 126\n",
      "Iter num 127\n",
      "Iter num 128\n",
      "Iter num 129\n",
      "Iter num 130\n",
      "Iter num 131\n",
      "Iter num 132\n",
      "Iter num 133\n",
      "Iter num 134\n",
      "Iter num 135\n",
      "Iter num 136\n",
      "Iter num 137\n",
      "Iter num 138\n",
      "Iter num 139\n",
      "Iter num 140\n",
      "Iter num 141\n",
      "Iter num 142\n",
      "Iter num 143\n",
      "Iter num 144\n",
      "Iter num 145\n",
      "Iter num 146\n",
      "Iter num 147\n",
      "Iter num 148\n",
      "Iter num 149\n",
      "Iter num 150\n",
      "Iter num 151\n",
      "Iter num 152\n",
      "Iter num 153\n",
      "Iter num 154\n",
      "Iter num 155\n",
      "Iter num 156\n",
      "Iter num 157\n",
      "Iter num 158\n",
      "Iter num 159\n",
      "Iter num 160\n",
      "Iter num 161\n",
      "Iter num 162\n",
      "Iter num 163\n",
      "Iter num 164\n",
      "Iter num 165\n",
      "Iter num 166\n",
      "Iter num 167\n",
      "Iter num 168\n",
      "Iter num 169\n",
      "Iter num 170\n",
      "Iter num 171\n",
      "Iter num 172\n",
      "Iter num 173\n",
      "Iter num 174\n",
      "Iter num 175\n",
      "Iter num 176\n",
      "Iter num 177\n",
      "Iter num 178\n",
      "Iter num 179\n",
      "Iter num 180\n",
      "Iter num 181\n",
      "Iter num 182\n",
      "Iter num 183\n",
      "Iter num 184\n",
      "Iter num 185\n",
      "Iter num 186\n",
      "Iter num 187\n",
      "Iter num 188\n",
      "Iter num 189\n",
      "Iter num 190\n",
      "Iter num 191\n",
      "Iter num 192\n",
      "Iter num 193\n",
      "Iter num 194\n",
      "Iter num 195\n",
      "Iter num 196\n",
      "Iter num 197\n",
      "Iter num 198\n",
      "Iter num 199\n",
      "Iter num 200\n",
      "Iter num 201\n",
      "Iter num 202\n",
      "Iter num 203\n",
      "Iter num 204\n",
      "Iter num 205\n",
      "Iter num 206\n",
      "Iter num 207\n",
      "Iter num 208\n",
      "Iter num 209\n",
      "Iter num 210\n",
      "Iter num 211\n",
      "Iter num 212\n",
      "Iter num 213\n",
      "Iter num 214\n",
      "Iter num 215\n",
      "Iter num 216\n",
      "Iter num 217\n",
      "Iter num 218\n",
      "Iter num 219\n",
      "Iter num 220\n",
      "Iter num 221\n",
      "Iter num 222\n",
      "Iter num 223\n",
      "Iter num 224\n",
      "Iter num 225\n",
      "Iter num 226\n",
      "Iter num 227\n",
      "Iter num 228\n",
      "Iter num 229\n",
      "Iter num 230\n",
      "Iter num 231\n",
      "Iter num 232\n",
      "Iter num 233\n",
      "Iter num 234\n",
      "Iter num 235\n",
      "Iter num 236\n",
      "Iter num 237\n",
      "Iter num 238\n",
      "Iter num 239\n",
      "Iter num 240\n",
      "Iter num 241\n",
      "Iter num 242\n",
      "Iter num 243\n",
      "Iter num 244\n",
      "Iter num 245\n",
      "Iter num 246\n",
      "Iter num 247\n",
      "Iter num 248\n",
      "Iter num 249\n",
      "Iter num 250\n",
      "Iter num 251\n",
      "Iter num 252\n",
      "Iter num 253\n",
      "Iter num 254\n",
      "Iter num 255\n",
      "Iter num 256\n",
      "Iter num 257\n",
      "Iter num 258\n",
      "Iter num 259\n",
      "Iter num 260\n",
      "Iter num 261\n",
      "Iter num 262\n",
      "Iter num 263\n",
      "Iter num 264\n",
      "Iter num 265\n",
      "Iter num 266\n",
      "Iter num 267\n",
      "Iter num 268\n",
      "Iter num 269\n",
      "Iter num 270\n",
      "Iter num 271\n",
      "Iter num 272\n",
      "Iter num 273\n",
      "Iter num 274\n",
      "Iter num 275\n",
      "Iter num 276\n",
      "Iter num 277\n",
      "Iter num 278\n",
      "Iter num 279\n",
      "Iter num 280\n",
      "Iter num 281\n",
      "Iter num 282\n",
      "Iter num 283\n",
      "Iter num 284\n",
      "Iter num 285\n",
      "Iter num 286\n",
      "Iter num 287\n",
      "Iter num 288\n",
      "Iter num 289\n",
      "Iter num 290\n",
      "Iter num 291\n",
      "Iter num 292\n",
      "Iter num 293\n",
      "Iter num 294\n",
      "Iter num 295\n",
      "Iter num 296\n",
      "Iter num 297\n",
      "Iter num 298\n",
      "Iter num 299\n"
     ]
    }
   ],
   "source": [
    "nz = 100\n",
    "batch_size = 16\n",
    "embedding_size = 10\n",
    "num_iters = 300 #43.2k images for style, 48k for genre\n",
    "\n",
    "for i in range(num_iters):\n",
    "    print (\"Iter num %i\"%(i))\n",
    "    for class_idx in range(embedding_size):\n",
    "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
    "        with torch.no_grad():\n",
    "                    fixed_noise = \\\n",
    "                        Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1))\n",
    "        noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "        noise.data.normal_(0, 1)\n",
    "        text_embeddings = torch.zeros((batch_size, embedding_size)).cuda()\n",
    "        text_embeddings[:, class_idx] = 1\n",
    "\n",
    "#         inputs = (text_embeddings, noise)\n",
    "\n",
    "        lr_fake, fake = netG(text_embeddings, noise)\n",
    "\n",
    "        for im_idx, im in enumerate(fake.data):\n",
    "            \n",
    "            vutils.save_image(\n",
    "                        im, '%s%s/%i_%i.png' %\n",
    "                        (image_dir, folders[class_idx], i, im_idx), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
